{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Chainer](http://chainer.org/) とはニューラルネットの実装を簡単にしたフレームワークです。\n",
    "\n",
    "* 今回は機械翻訳にニューラルネットを適用してみました。\n",
    "\n",
    "![](./pictures/Chainer.jpg)\n",
    "\n",
    "* 今回は機械翻訳を行っていただきます。\n",
    "\n",
    "機械翻訳は機械が言語を別の言語に翻訳するものです。\n",
    "\n",
    "機械翻訳にはいくつか種類があるのでここでも紹介しておきます。\n",
    "\n",
    "* PBMT(Phrase Base Machine Translation)モデル\n",
    " * [moses](http://www.statmt.org/moses/)というオープンソースで使用できるメジャーな機械翻訳のモデルですが、難しすぎて理解できない人を続出させる機械翻訳の鬼門です\n",
    "* ニューラル機械翻訳\n",
    " * 翻訳元単語の辞書ベクトルを潜在空間ベクトルに落とし込み、ニューラルネットで翻訳先言語を学習させる手法\n",
    "\n",
    "以下では、このChainerを利用しデータを準備するところから実際に言語モデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "1. [各種ライブラリ導入](#各種ライブラリ導入) \n",
    "2. [各値を設定](#各値を設定)\n",
    "3. [モデルの学習](#モデルの学習)\n",
    "4. [予測](#予測)\n",
    "5. [実行](#実行)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.各種ライブラリ導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの言語処理では多数のライブラリを導入します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from chainer import functions, optimizers\n",
    "\n",
    "import util.generators as gens\n",
    "from util.functions import trace, fill_batch\n",
    "from util.model_file import ModelFile\n",
    "from util.vocabulary import Vocabulary\n",
    "\n",
    "from util.chainer_cpu_wrapper import wrapper\n",
    "\n",
    "from EncoderDecoderModel import EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`導入するライブラリの代表例は下記です。\n",
    "\n",
    "* `numpy`: 行列計算などの複雑な計算を行なうライブラリ\n",
    "* `chainer`: Chainerの導入\n",
    "* `util`:今回の処理で必要なライブラリが入っています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.機械翻訳のクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記を設定しています。\n",
    "* ニューラルネットを用いて機械翻訳用のモデルを構成しています。\n",
    "ややこしいので各構成の説明\n",
    "\n",
    "全体構成\n",
    "\n",
    "![](./pictures/chainer_machine_translation1.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.各値を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各値を設定\n",
    "\n",
    "* モードを学習かテストか設定\n",
    "* 翻訳元言語の設定\n",
    "* 翻訳先言語の設定\n",
    "* 語彙の設定\n",
    "* 潜在空間の設定\n",
    "* 隠れ層の設定\n",
    "* 学習回数の設定\n",
    "* ミニバッチサイズの設定\n",
    "* 最大予測言語数の設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "source = \"source_wakati_mecab.txt\"\n",
    "target = \"target.txt\"\n",
    "vocab = 32768\n",
    "embed = 256\n",
    "hidden = 512\n",
    "epoch = 100\n",
    "minibatch = 64\n",
    "generation_limit = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習用のメソッド\n",
    "\n",
    "* 翻訳元言語を処理用の変数に変換\n",
    "* 翻訳先言語を処理用の変数に変換\n",
    "* 学習用のモデル設定\n",
    "\n",
    "学習回数分、下記の処理を行う\n",
    "\n",
    "* 翻訳元言語をlist化\n",
    "* 翻訳先言語をlist化\n",
    "* sorted_parellel処理はややこしいので少し解説\n",
    "　翻訳元言語と翻訳先言語のリストを100×ミニバッチのサイズ分渡すとタプル形式でソートして返してくれます。\n",
    "　それをbatch関数でミニバッチのサイズ分取得しているのがgen3の処理です。\n",
    " \n",
    "* 初期化\n",
    "* gen3を用いて翻訳元言語と翻訳先言語を取り出し\n",
    "　すべての文字列の末尾に\"</s>\"を挿入する\n",
    "\n",
    "* 仮説候補を取得\n",
    "* 翻訳元言語、翻訳先言語、翻訳仮説を表示\n",
    "* 各学習ごとにモデルを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    trace('making vocaburaries ...')\n",
    "    src_vocab = Vocabulary.new(gens.word_list(source), vocab)\n",
    "    trg_vocab = Vocabulary.new(gens.word_list(target), vocab)\n",
    "\n",
    "    trace('making model ...')\n",
    "    model = EncoderDecoderModel.new(src_vocab, trg_vocab, embed, hidden)\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "        trace('epoch %d/%d: ' % (i_epoch + 1, epoch))\n",
    "        trained = 0\n",
    "        gen1 = gens.word_list(source)\n",
    "        gen2 = gens.word_list(target)\n",
    "        gen3 = gens.batch(gens.sorted_parallel(gen1, gen2, 100 * minibatch), minibatch)\n",
    "        model.init_optimizer()\n",
    "\n",
    "        for src_batch, trg_batch in gen3:\n",
    "            src_batch = fill_batch(src_batch)\n",
    "            trg_batch = fill_batch(trg_batch)\n",
    "            K = len(src_batch)\n",
    "            hyp_batch = model.train(src_batch, trg_batch)\n",
    "\n",
    "            for k in range(K):\n",
    "                trace('epoch %3d/%3d, sample %8d' % (i_epoch + 1, epoch, trained + k + 1))\n",
    "                trace('  src = ' + ' '.join([x if x != '</s>' else '*' for x in src_batch[k]]))\n",
    "                trace('  trg = ' + ' '.join([x if x != '</s>' else '*' for x in trg_batch[k]]))\n",
    "                trace('  hyp = ' + ' '.join([x if x != '</s>' else '*' for x in hyp_batch[k]]))\n",
    "\n",
    "            trained += K\n",
    "\n",
    "        trace('saving model ...')\n",
    "        model.save(model + '.%03d' % (epoch + 1))\n",
    "\n",
    "    trace('finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測\n",
    "\n",
    "* 学習したモデルを読み込む\n",
    "* 翻訳元言語をミニバッチのサイズ分読み込んで、仮説候補をモデルから予測\n",
    "* 仮説候補を表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(args):\n",
    "    trace('loading model ...')\n",
    "    model = EncoderDecoderModel.load(model)\n",
    "    \n",
    "    trace('generating translation ...')\n",
    "    generated = 0\n",
    "\n",
    "    with open(target, 'w') as fp:\n",
    "        for src_batch in gens.batch(gens.word_list(source), minibatch):\n",
    "            src_batch = fill_batch(src_batch)\n",
    "            K = len(src_batch)\n",
    "\n",
    "            trace('sample %8d - %8d ...' % (generated + 1, generated + K))\n",
    "            hyp_batch = model.predict(src_batch, generation_limit)\n",
    "\n",
    "            for hyp in hyp_batch:\n",
    "                hyp.append('</s>')\n",
    "                hyp = hyp[:hyp.index('</s>')]\n",
    "                print(' '.join(hyp), file=fp)\n",
    "\n",
    "            generated += K\n",
    "\n",
    "    trace('finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    trace('initializing ...')\n",
    "    wrapper.init()\n",
    "\n",
    "    if mode == 'train': train_model()\n",
    "    elif mode == 'test': test_model()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
