{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Chainer](http://chainer.org/) とはニューラルネットの実装を簡単にしたフレームワークです。\n",
    "\n",
    "* 今回は機械翻訳にニューラルネットを適用してみました。\n",
    "\n",
    "![](./pictures/Chainer.jpg)\n",
    "\n",
    "* 今回は機械翻訳を行っていただきます。\n",
    "\n",
    "機械翻訳は機械が言語を別の言語に翻訳するものです。\n",
    "\n",
    "機械翻訳にはいくつか種類があるのでここでも紹介しておきます。\n",
    "\n",
    "* IBMモデル\n",
    " * [moses](http://www.statmt.org/moses/)というオープンソースで使用できるメジャーな機械翻訳のモデルですが、難しすぎて理解できない人を続出させる機械翻訳の鬼門です\n",
    "* ニューラル機械翻訳\n",
    " * 翻訳元単語の辞書ベクトルを潜在空間ベクトルに落とし込み、ニューラルネットで翻訳先言語を学習させる手法\n",
    "\n",
    "以下では、このChainerを利用しデータを準備するところから実際に言語モデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "1. [各種ライブラリ導入](#各種ライブラリ導入) \n",
    "2. [機械翻訳のクラス](#機械翻訳のクラス) \n",
    "3. [クラスの初期設定](#クラスの初期設定)\n",
    "4. [モデルの保存](#モデルの保存) \n",
    "5. [モデルの読み込み](#モデルの読み込み)\n",
    "6. [初期設定とForWard処理(Encoding Decoding)](#初期設定とForWard処理(Encoding Decoding))\n",
    "7. [モデルの学習と予測のメソッドを実装](#モデルの学習と予測のメソッドを実装)\n",
    "8. [各値を設定](#各値を設定)\n",
    "9. [モデルの学習](#モデルの学習)\n",
    "10. [予測](#予測)\n",
    "11. [実行](#実行)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.各種ライブラリ導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの言語処理では多数のライブラリを導入します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from chainer import functions, optimizers\n",
    "\n",
    "import util.generators as gens\n",
    "from util.functions import trace, fill_batch\n",
    "from util.model_file import ModelFile\n",
    "from util.vocabulary import Vocabulary\n",
    "\n",
    "from util.chainer_cpu_wrapper import wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`導入するライブラリの代表例は下記です。\n",
    "\n",
    "* `numpy`: 行列計算などの複雑な計算を行なうライブラリ\n",
    "* `chainer`: Chainerの導入\n",
    "* `util`:今回の処理で必要なライブラリが入っています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.機械翻訳のクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記を設定しています。\n",
    "* ニューラルネットを用いて機械翻訳用のモデルを構成しています。\n",
    "ややこしいので各構成の説明\n",
    "\n",
    "全体構成\n",
    "\n",
    "![](./pictures/chainer_machine_translation1.png)\n",
    "\n",
    "Encoder部\n",
    "1. 翻訳元言語のBag of Wordsを潜在ベクトル空間に写像\n",
    "2. 潜在ベクトル空間の値を隠れ層に遷移\n",
    "3. 隠れ層から赤くなっている隠れ層まで遷移\n",
    "\n",
    "Decoder部\n",
    "4. 赤くなっている隠れ層からDecoder部の隠れ層で値を受け取る\n",
    "5. 隠れ層から値を潜在ベクトル空間に写像\n",
    "6. 潜在ベクトル空間から翻訳先言語のBag of Wordsに写像\n",
    "7. 翻訳先言語のBag of Wordsに写像した値を次の単語の隠れ層に入れる\n",
    "8. 隠れ層から隠れ層への遷移\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelMakeModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make_model(self):\n",
    "        self.model = wrapper.make_model(\n",
    "            # encoder\n",
    "            w_xi = functions.EmbedID(len(self.src_vocab), self.n_embed),       #1\n",
    "            w_ip = functions.Linear(self.n_embed, 4 * self.n_hidden),          #2\n",
    "            w_pp = functions.Linear(self.n_hidden, 4 * self.n_hidden),         #3\n",
    "            # decoder\n",
    "            w_pq = functions.Linear(self.n_hidden, 4 * self.n_hidden),         #4\n",
    "            w_qj = functions.Linear(self.n_hidden, self.n_embed),              #5\n",
    "            w_jy = functions.Linear(self.n_embed, len(self.trg_vocab)),        #6\n",
    "            w_yq = functions.EmbedID(len(self.trg_vocab), 4 * self.n_hidden),  #7\n",
    "            w_qq = functions.Linear(self.n_hidden, 4 * self.n_hidden),         #8\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.クラスの初期設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスの初期設定を行っています。\n",
    "\n",
    "* クラスの生成\n",
    "* 翻訳元言語の登録\n",
    "* 翻訳先言語の登録\n",
    "* 潜在ベクトル数の登録\n",
    "* 隠れ層の登録\n",
    "* ニューラル翻訳モデルの生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelNew(EncoderDecoderModelMakeModel):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def new(src_vocab, trg_vocab, n_embed, n_hidden):\n",
    "        self = EncoderDecoderModel()\n",
    "        print(dir(self))\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.make_model()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.モデルの保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが出力する各値を保存します。\n",
    "\n",
    "* 翻訳元言語の語彙数\n",
    "* 翻訳先言語の語彙数\n",
    "* 潜在空間ベクトル\n",
    "* 隠れ層\n",
    "* モデル\n",
    "* モデルが保持する各値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelSave(EncoderDecoderModelNew):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def save(self, filename):\n",
    "        with ModelFile(filename, 'w') as fp:\n",
    "            self.src_vocab.save(fp.get_file_pointer())\n",
    "            self.trg_vocab.save(fp.get_file_pointer())\n",
    "            fp.write(self.n_embed)\n",
    "            fp.write(self.n_hidden)\n",
    "            wrapper.begin_model_access(self.model)\n",
    "            fp.write_embed(self.model.w_xi)\n",
    "            fp.write_linear(self.model.w_ip)\n",
    "            fp.write_linear(self.model.w_pp)\n",
    "            fp.write_linear(self.model.w_pq)\n",
    "            fp.write_linear(self.model.w_qj)\n",
    "            fp.write_linear(self.model.w_jy)\n",
    "            fp.write_embed(self.model.w_yq)\n",
    "            fp.write_linear(self.model.w_qq)\n",
    "            wrapper.end_model_access(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.モデルの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを読み込む際の値の定義です。\n",
    "\n",
    "* 翻訳元言語の語彙数\n",
    "* 翻訳先言語の語彙数\n",
    "* 潜在空間ベクトル\n",
    "* 隠れ層\n",
    "* モデル\n",
    "* モデルが保持する各値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelLoad(EncoderDecoderModelSave):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        self = EncoderDecoderModel()\n",
    "        with ModelFile(filename) as fp:\n",
    "            self.src_vocab = Vocabulary.load(fp.get_file_pointer())\n",
    "            self.trg_vocab = Vocabulary.load(fp.get_file_pointer())\n",
    "            self.n_embed = int(fp.read())\n",
    "            self.n_hidden = int(fp.read())\n",
    "            self.make_model()\n",
    "            wrapper.begin_model_access(self.model)\n",
    "            fp.read_embed(self.model.w_xi)\n",
    "            fp.read_linear(self.model.w_ip)\n",
    "            fp.read_linear(self.model.w_pp)\n",
    "            fp.read_linear(self.model.w_pq)\n",
    "            fp.read_linear(self.model.w_qj)\n",
    "            fp.read_linear(self.model.w_jy)\n",
    "            fp.read_embed(self.model.w_yq)\n",
    "            fp.read_linear(self.model.w_qq)\n",
    "            wrapper.end_model_access(self.model)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.初期設定とForWard処理(Encoding Decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化のための初期設定とForWard処理の記述です。\n",
    "\n",
    "最適化のための初期設定\n",
    "\n",
    "* 最適化手法の設定（AdaGrad)\n",
    "* モデル設定\n",
    "\n",
    "ForWard処理\n",
    "\n",
    "* 初期設定\n",
    "\n",
    "Encoding処理\n",
    "\n",
    "* 入力の値を潜在ベクトル空間に写像\n",
    "* 潜在ベクトル空間のデータを隠れ層用のデータに変換\n",
    "* 隠れ層のデータをLSTMで渡す\n",
    "\n",
    "Decoding処理\n",
    "\n",
    "学習処理\n",
    "\n",
    "* 隠れ層から潜在ベクトル空間への写像処理\n",
    "* 潜在ベクトル空間からBag Of Wordsへ写像\n",
    "* ミニバッチのサイズ分、翻訳先言語を保持\n",
    "* 正解の翻訳結果と予測した結果を照らし合わせて損失を計算\n",
    "* 翻訳結果を保持\n",
    "* 翻訳結果を仮説候補として追加\n",
    "* 翻訳結果を潜在ベクトル空間に写像した値と隠れ層から伝搬した値を合わして次の隠れ層に伝搬させていく\n",
    "\n",
    "予測\n",
    "\n",
    "* 生成制限以下の範囲で予測を行う\n",
    "* </s>まで予測したら終了するようになっている\n",
    "* 学習とは違うので単純に予測したデータをlstmに加えている\n",
    "* それ以外の処理は損失計算部分がない点以外同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelInitForward(EncoderDecoderModelLoad):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def init_optimizer(self):\n",
    "        self.opt = optimizers.AdaGrad(lr=0.01)\n",
    "        self.opt.setup(self.model.collect_parameters())\n",
    "\n",
    "    def forward(self, is_training, src_batch, trg_batch = None, generation_limit = None):\n",
    "        m = self.model\n",
    "        tanh = functions.tanh\n",
    "        lstm = functions.lstm\n",
    "        batch_size = len(src_batch)\n",
    "        src_len = len(src_batch[0])\n",
    "        src_stoi = self.src_vocab.stoi\n",
    "        trg_stoi = self.trg_vocab.stoi\n",
    "        trg_itos = self.trg_vocab.itos\n",
    "        s_c = wrapper.zeros((batch_size, self.n_hidden))\n",
    "        \n",
    "        # encoding\n",
    "        s_x = wrapper.make_var([src_stoi('</s>') for _ in range(batch_size)], dtype=np.int32)\n",
    "        s_i = tanh(m.w_xi(s_x))\n",
    "        s_c, s_p = lstm(s_c, m.w_ip(s_i))\n",
    "\n",
    "        for l in reversed(range(src_len)):\n",
    "            s_x = wrapper.make_var([src_stoi(src_batch[k][l]) for k in range(batch_size)], dtype=np.int32)\n",
    "            s_i = tanh(m.w_xi(s_x))\n",
    "            s_c, s_p = lstm(s_c, m.w_ip(s_i) + m.w_pp(s_p))\n",
    "\n",
    "        s_c, s_q = lstm(s_c, m.w_pq(s_p))\n",
    "        hyp_batch = [[] for _ in range(batch_size)]\n",
    "        \n",
    "        # decoding\n",
    "        if is_training:\n",
    "            accum_loss = wrapper.zeros(())\n",
    "            trg_len = len(trg_batch[0])\n",
    "            \n",
    "            for l in range(trg_len):\n",
    "                s_j = tanh(m.w_qj(s_q))\n",
    "                r_y = m.w_jy(s_j)\n",
    "                s_t = wrapper.make_var([trg_stoi(trg_batch[k][l]) for k in range(batch_size)], dtype=np.int32)\n",
    "                accum_loss += functions.softmax_cross_entropy(r_y, s_t)\n",
    "                output = wrapper.get_data(r_y).argmax(1)\n",
    "\n",
    "                for k in range(batch_size):\n",
    "                    hyp_batch[k].append(trg_itos(output[k]))\n",
    "\n",
    "                s_c, s_q = lstm(s_c, m.w_yq(s_t) + m.w_qq(s_q))\n",
    "\n",
    "            return hyp_batch, accum_loss\n",
    "        else:\n",
    "            while len(hyp_batch[0]) < generation_limit:\n",
    "                s_j = tanh(m.w_qj(s_q))\n",
    "                r_y = m.w_jy(s_j)\n",
    "                output = wrapper.get_data(r_y).argmax(1)\n",
    "\n",
    "                for k in range(batch_size):\n",
    "                    hyp_batch[k].append(trg_itos(output[k]))\n",
    "\n",
    "                if all(hyp_batch[k][-1] == '</s>' for k in range(batch_size)): break\n",
    "\n",
    "                s_y = wrapper.make_var(output, dtype=np.int32)\n",
    "                s_c, s_q = lstm(s_c, m.w_yq(s_y) + m.w_qq(s_q))\n",
    "            \n",
    "            return hyp_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.モデルの学習と予測のメソッドを実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習と予測処理の実装です。\n",
    "\n",
    "学習\n",
    "\n",
    "* 初期化\n",
    "* Forward処理で仮説候補と損失を算出\n",
    "* backward処理で損失を計算\n",
    "* 最適化における発散の防止\n",
    "* 値を最適化\n",
    "\n",
    "予測\n",
    "\n",
    "* Forward処理に記述した予測処理を行います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModel(EncoderDecoderModelInitForward):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, src_batch, trg_batch):\n",
    "        self.opt.zero_grads()\n",
    "        hyp_batch, accum_loss = self.forward(True, src_batch, trg_batch=trg_batch)\n",
    "        accum_loss.backward()\n",
    "        self.opt.clip_grads(10)\n",
    "        self.opt.update()\n",
    "        return hyp_batch\n",
    "\n",
    "    def predict(self, src_batch, generation_limit):\n",
    "        return self.forward(False, src_batch, generation_limit=generation_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.各値を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各値を設定\n",
    "\n",
    "* モードを学習かテストか設定\n",
    "* 翻訳元言語の設定\n",
    "* 翻訳先言語の設定\n",
    "* 語彙の設定\n",
    "* 潜在空間の設定\n",
    "* 隠れ層の設定\n",
    "* 学習回数の設定\n",
    "* ミニバッチサイズの設定\n",
    "* 最大予測言語数の設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "source = \"source.txt\"\n",
    "target = \"target.txt\"\n",
    "vocab = 32768\n",
    "embed = 256\n",
    "hidden = 512\n",
    "epoch = 100\n",
    "minibatch = 64\n",
    "generation_limit = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習用のメソッド\n",
    "\n",
    "* 翻訳元言語を処理用の変数に変換\n",
    "* 翻訳先言語を処理用の変数に変換\n",
    "* 学習用のモデル設定\n",
    "\n",
    "学習回数分、下記の処理を行う\n",
    "\n",
    "* 翻訳元言語をlist化\n",
    "* 翻訳先言語をlist化\n",
    "* sorted_parellel処理はややこしいので少し解説\n",
    "　翻訳元言語と翻訳先言語のリストを100×ミニバッチのサイズ分渡すとタプル形式でソートして返してくれます。\n",
    "　それをbatch関数でミニバッチのサイズ分取得しているのがgen3の処理です。\n",
    " \n",
    "* 初期化\n",
    "* gen3を用いて翻訳元言語と翻訳先言語を取り出し\n",
    "　すべての文字列の末尾に\"</s>\"を挿入する\n",
    "\n",
    "* 仮説候補を取得\n",
    "* 翻訳元言語、翻訳先言語、翻訳仮説を表示\n",
    "* 各学習ごとにモデルを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    trace('making vocaburaries ...')\n",
    "    src_vocab = Vocabulary.new(gens.word_list(source), vocab)\n",
    "    trg_vocab = Vocabulary.new(gens.word_list(target), vocab)\n",
    "\n",
    "    trace('making model ...')\n",
    "    model = EncoderDecoderModel.new(src_vocab, trg_vocab, embed, hidden)\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "        trace('epoch %d/%d: ' % (i_epoch + 1, epoch))\n",
    "        trained = 0\n",
    "        gen1 = gens.word_list(source)\n",
    "        gen2 = gens.word_list(target)\n",
    "        gen3 = gens.batch(gens.sorted_parallel(gen1, gen2, 100 * minibatch), minibatch)\n",
    "        model.init_optimizer()\n",
    "\n",
    "        for src_batch, trg_batch in gen3:\n",
    "            src_batch = fill_batch(src_batch)\n",
    "            trg_batch = fill_batch(trg_batch)\n",
    "            K = len(src_batch)\n",
    "            hyp_batch = model.train(src_batch, trg_batch)\n",
    "\n",
    "            for k in range(K):\n",
    "                trace('epoch %3d/%3d, sample %8d' % (i_epoch + 1, epoch, trained + k + 1))\n",
    "                trace('  src = ' + ' '.join([x if x != '</s>' else '*' for x in src_batch[k]]))\n",
    "                trace('  trg = ' + ' '.join([x if x != '</s>' else '*' for x in trg_batch[k]]))\n",
    "                trace('  hyp = ' + ' '.join([x if x != '</s>' else '*' for x in hyp_batch[k]]))\n",
    "\n",
    "            trained += K\n",
    "\n",
    "        trace('saving model ...')\n",
    "        model.save(model + '.%03d' % (epoch + 1))\n",
    "\n",
    "    trace('finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測\n",
    "\n",
    "* 学習したモデルを読み込む\n",
    "* 翻訳元言語をミニバッチのサイズ分読み込んで、仮説候補をモデルから予測\n",
    "* 仮説候補を表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(args):\n",
    "    trace('loading model ...')\n",
    "    model = EncoderDecoderModel.load(model)\n",
    "    \n",
    "    trace('generating translation ...')\n",
    "    generated = 0\n",
    "\n",
    "    with open(target, 'w') as fp:\n",
    "        for src_batch in gens.batch(gens.word_list(source), minibatch):\n",
    "            src_batch = fill_batch(src_batch)\n",
    "            K = len(src_batch)\n",
    "\n",
    "            trace('sample %8d - %8d ...' % (generated + 1, generated + K))\n",
    "            hyp_batch = model.predict(src_batch, generation_limit)\n",
    "\n",
    "            for hyp in hyp_batch:\n",
    "                hyp.append('</s>')\n",
    "                hyp = hyp[:hyp.index('</s>')]\n",
    "                print(' '.join(hyp), file=fp)\n",
    "\n",
    "            generated += K\n",
    "\n",
    "    trace('finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    trace('initializing ...')\n",
    "    wrapper.init()\n",
    "\n",
    "    if mode == 'train': train_model()\n",
    "    elif mode == 'test': test_model()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
