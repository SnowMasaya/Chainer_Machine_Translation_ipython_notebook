{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Chainer](http://chainer.org/) とはニューラルネットの実装を簡単にしたフレームワークです。\n",
    "\n",
    "* 今回は機械翻訳にニューラルネットを適用してみました。\n",
    "\n",
    "![](./pictures/Chainer.jpg)\n",
    "\n",
    "* 今回は機械翻訳を行っていただきます。\n",
    "\n",
    "\n",
    "機械翻訳は機械が言語を別の言語に翻訳するものです。\n",
    "\n",
    "機械翻訳にはいくつか種類があるのでここでも紹介しておきます。\n",
    "\n",
    "* PBMT(Phrase Base Machine Translation)モデル\n",
    " * [moses](http://www.statmt.org/moses/)というオープンソースで使用できるメジャーな機械翻訳のモデルですが、難しすぎて理解できない人を続出させる機械翻訳の鬼門です\n",
    "* ニューラル機械翻訳\n",
    " * 翻訳元単語の辞書ベクトルを潜在空間ベクトルに落とし込み、ニューラルネットで翻訳先言語を学習させる手法\n",
    "\n",
    "以下では、このChainerを利用しデータを準備するところから実際にNN翻訳モデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "<A HREF=#1.各種ライブラリ導入 >1.各種ライブラリ導入</A><br>\n",
    "<A HREF=#2.機械翻訳のクラス >2.機械翻訳のクラス</A><br>\n",
    "<A HREF=#3.翻訳処理を行うforwardに必要なパラメータ設定 >3.翻訳処理を行うforwardに必要なパラメータ設定</A><br>\n",
    "<A HREF=#4.翻訳処理を行うEncoder処理部分 >4.翻訳処理を行うEncoder処理部分</A><br>\n",
    "<A HREF=#5.翻訳処理を行うDecoder処理部分 >5.翻訳処理を行うDecoder処理部分</A><br>\n",
    "<A HREF=#6.翻訳処理を行うforward処理部分 >6.翻訳処理を行うforward処理部分</A><br>\n",
    "<A HREF=#7.各値を設定 >7.各値を設定</A><br>\n",
    "<A HREF=#8.実行 >8.実行</A><br>\n",
    "<A HREF=#9.学習したモデルを使用したテスト >9.学習したモデルを使用したテスト</A><br>\n",
    "<A HREF=#10.学習したモデルを評価 (Advanced) >10.学習したモデルを評価 (Advanced)</A><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=1.各種ライブラリ導入 /> 1.各種ライブラリ導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの言語処理では多数のライブラリを導入します。\n",
    "Ctrl → m → lをコードの部分で入力すると行番号が出ます。ハンズオンの都合上、行番号があった方が良いので対応よろしくお願いします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#表示用に使用しています。\n",
    "from util.functions import trace\n",
    "import numpy as np\n",
    "\n",
    "from chainer import functions, optimizers\n",
    "\n",
    "#cpu計算とgpu計算で使い分けるラッパー\n",
    "from util.chainer_cpu_wrapper import wrapper\n",
    "\n",
    "from EncoderDecoderModel import EncoderDecoderModel\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`導入するライブラリの代表例は下記です。\n",
    "\n",
    "* `numpy`: 行列計算などの複雑な計算を行なうライブラリ\n",
    "* `chainer`: Chainerの導入\n",
    "* `util`:今回の処理で必要なライブラリが入っています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=2.機械翻訳のクラス /> 2.機械翻訳のクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記を設定しています。\n",
    "* ニューラルネットを用いて機械翻訳用のモデルを構成しています。\n",
    "\n",
    "全体構成\n",
    "\n",
    "![](./pictures/NN_machine_translation.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=3.翻訳処理を行うforwardに必要なパラメータ設定 /> 3.翻訳処理を行うforwardに必要なパラメータ設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のコードで必要なパラメータを設定するクラスを定義しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelParameter():\n",
    "    \n",
    "    def __init__(self, is_training, src_batch, encoderDecoderModel, trg_batch = None, generation_limit = None):\n",
    "        self.model = encoderDecoderModel.model\n",
    "        self.tanh = functions.tanh\n",
    "        self.lstm = functions.lstm\n",
    "        self.batch_size = len(src_batch)\n",
    "        self.src_len = len(src_batch[0])\n",
    "        #翻訳元言語を単語からインデックスにしている（ニューラルネットの空間で扱うため）\n",
    "        self.src_stoi = encoderDecoderModel.src_vocab.stoi\n",
    "        #翻訳先言語を単語からインデックスにしている（ニューラルネットの空間で扱うため）\n",
    "        self.trg_stoi = encoderDecoderModel.trg_vocab.stoi\n",
    "        #翻訳先言語をインデックスから単語にしている(翻訳結果として保持するため、翻訳先言語だけ用意している)\n",
    "        self.trg_itos = encoderDecoderModel.trg_vocab.itos\n",
    "        #lstmのために状態を初期化\n",
    "        self.state_c = wrapper.zeros((self.batch_size, encoderDecoderModel.n_hidden))\n",
    "        self.trg_batch = trg_batch\n",
    "        self.generation_limit = generation_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=4.翻訳処理を行うEncoder処理部分 /> 4.翻訳処理を行うEncoder処理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記の論文を参考にしてforward処理を記述しています。\n",
    "\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "\n",
    "* Encoder部分\n",
    "もっとも特徴的な部分は翻訳元言語を逆順にしていることです。そうすることで精度が向上していると述べており、今回の翻訳のNNモデルもそれを再現しています。\n",
    "\n",
    "この論文でははっきりした要因はわかっていないが、おそらく翻訳先の言語と翻訳元言語の距離が逆順にすることで最初の単語の距離が近くなり、翻訳のタイムラグが少なくなったことが起因していると考えられています。\n",
    "\n",
    "![](./pictures/encoder.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelEncoding():\n",
    "    \n",
    "    def encoding(self, src_batch, parameter, trg_batch = None, generation_limit = None):\n",
    "\n",
    "#--------Hands on------------------------------------------------------------------#\n",
    "    # encoding\n",
    "        #翻訳元言語の末尾</s>を潜在空間に射像し、隠れ層に入力、lstmで出力までをバッチサイズ分行う\n",
    "        #予め末尾の設定をしていないと終了単語が分からないため\n",
    "        #1:翻訳元言語の入力x:図のx部分に相当\n",
    "        state_x = wrapper.make_var([parameter.src_stoi('</s>') for _ in range(parameter.batch_size)], dtype=np.int32)\n",
    "        #2:翻訳元言語の入力xを潜在空間に射像する。（次元数を圧縮するため）:図のi部分に相当\n",
    "        state_i = parameter.tanh(parameter.model.weight_xi(state_x))\n",
    "        #3:潜在空間iの入力をlstmに入力し、次の単語予測に使用する:図のp部分に相当\n",
    "        parameter.status_c, state_p = parameter.lstm(parameter.state_c, parameter.model.weight_ip(state_i))\n",
    "        \n",
    "        #翻訳元言語を逆順に上記と同様の処理を行う   \n",
    "        for l in reversed(range(parameter.src_len)):\n",
    "            #翻訳元言語を語彙空間に写像\n",
    "            state_x = wrapper.make_var([parameter.src_stoi(src_batch[k][l]) for k in range(parameter.batch_size)], \n",
    "                                       dtype=np.int32)\n",
    "            #語彙空間を潜在空間（次元数が減る）に射像\n",
    "            state_i = parameter.tanh(parameter.model.weight_xi(state_x))\n",
    "            #状態と出力結果をlstmにより出力。lstmの入力には前の状態と語彙空間の重み付き出力と前回の重み付き出力を入力としている\n",
    "            parameter.state_c, state_p = parameter.lstm(parameter.status_c, parameter.model.weight_ip(state_i) \n",
    "                                                        + parameter.model.weight_pp(state_p))\n",
    "\n",
    "        #次のミニバッチ処理のために最終結果をlstmで出力。翻訳の仮説用のリストを保持\n",
    "        parameter.state_c, state_q = parameter.lstm(parameter.state_c, parameter.model.weight_pq(state_p))\n",
    "        hyp_batch = [[] for _ in range(parameter.batch_size)]\n",
    "        return state_q, hyp_batch\n",
    "#--------Hands on------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=5.翻訳処理を行うDecoder処理部分 /> 5.翻訳処理を行うDecoder処理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decoder部分\n",
    "\n",
    "学習部分と予測部分を実装しています。学習部分ではターゲット先の単語の取得と損失の計算をしています。\n",
    "またlstmで次回の学習に使用する部分では学習では正解の翻訳、予測では予測した翻訳を使用しています。\n",
    "\n",
    "![](./pictures/decorder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelDecoding():\n",
    "    \n",
    "    def decoding(self, is_training, src_batch, parameter, state_q, hyp_batch, trg_batch = None, generation_limit = None):\n",
    "\n",
    "#--------Hands on------------------------------------------------------------------#\n",
    "    # decoding\n",
    "        \"\"\"\n",
    "　　     学習\n",
    "        \"\"\"\n",
    "        if is_training:\n",
    "            #損失の初期化及び答えとなる翻訳先言語の長さを取得。（翻訳元言語と翻訳先言語で長さが異なるため）\n",
    "            #損失が最小となるように学習するため必要\n",
    "            accum_loss = wrapper.zeros(())\n",
    "            trg_len = len(parameter.trg_batch[0])\n",
    "\n",
    "            #ニューラルネットの処理は基本的にEncodingと同一であるが、損失計算と翻訳仮説候補の確保の処理が加わっている\n",
    "            for l in range(trg_len):\n",
    "                #1:翻訳元言語に対するニューラルの出力qを受け取り、潜在空間jに射像\n",
    "                state_j = parameter.tanh(parameter.model.weight_qj(state_q))\n",
    "                #2:潜在空間jから翻訳先言語yの空間に射像\n",
    "                result_y = parameter.model.weight_jy(state_j)\n",
    "                #3:答えとなる翻訳結果を取得\n",
    "                state_target = wrapper.make_var([parameter.trg_stoi(parameter.trg_batch[k][l]) \n",
    "                                                 for k in range(parameter.batch_size)], dtype=np.int32)\n",
    "                #答えと翻訳結果により損失を計算\n",
    "                accum_loss += functions.softmax_cross_entropy(result_y, state_target)\n",
    "                #複数翻訳候補が出力されるため、出力にはもっとも大きな値を選択\n",
    "                output = wrapper.get_data(result_y).argmax(1)\n",
    "\n",
    "                #翻訳仮説確保(インデックスから翻訳単語に直す処理も行っている）\n",
    "                for k in range(parameter.batch_size):\n",
    "                    hyp_batch[k].append(parameter.trg_itos(output[k]))\n",
    "\n",
    "                #状態と出力結果をlstmにより出力。lstmの入力には前の状態と語彙空間の重み付き出力と前回の重み付き出力を入力としている\n",
    "                parameter.status_c, state_q = parameter.lstm(parameter.status_c, parameter.model.weight_yq(state_target) \n",
    "                                                             + parameter.model.weight_qq(state_q))\n",
    "            return hyp_batch, accum_loss\n",
    "        else:\n",
    "            \"\"\"\n",
    "            予測部分\n",
    "            \"\"\"\n",
    "            #末尾に</s>が予測できないと無限に翻訳してしまうため、予測では予測する翻訳言語の長さに制約をしている\n",
    "            while len(hyp_batch[0]) < parameter.generation_limit:\n",
    "                state_j = parameter.tanh(parameter.model.weight_qj(state_q))\n",
    "                result_y = parameter.model.weight_jy(state_j)\n",
    "                #複数翻訳候補が出力されるため、出力にはもっとも大きな値を選択\n",
    "                output = wrapper.get_data(result_y).argmax(1)\n",
    "\n",
    "                #翻訳仮説確保(インデックスから翻訳単語に直す処理も行っている）\n",
    "                for k in range(parameter.batch_size):\n",
    "                    hyp_batch[k].append(parameter.trg_itos(output[k]))\n",
    "\n",
    "                #ミニバッチサイズ分の翻訳仮説の末尾が</s>になったときにDecoding処理が終わるようになっている。\n",
    "                if all(hyp_batch[k][-1] == '</s>' for k in range(parameter.batch_size)): break\n",
    "                \n",
    "                #翻訳仮説をニューラルネットで扱える空間に射像している\n",
    "                state_y = wrapper.make_var(output, dtype=np.int32)\n",
    "                #次のlstmの処理のために出力結果と状態を渡している\n",
    "                parameter.status_c, state_q = parameter.lstm(parameter.state_c, parameter.model.weight_yq(state_y) \n",
    "                                                             + parameter.model.weight_qq(state_q))\n",
    "\n",
    "            return hyp_batch\n",
    "        \n",
    "#--------Hands on------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=6.翻訳処理を行うforward処理部分 /> 6.翻訳処理を行うforward処理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の処理を実行するためのメソッドです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelForward(EncoderDecoderModel):\n",
    "    \n",
    "    def forward(self, is_training, src_batch, trg_batch = None, generation_limit = None):\n",
    "    #パラメータ設定\n",
    "        parameter = EncoderDecoderModelParameter(is_training, src_batch, self, trg_batch, generation_limit)\n",
    "        \n",
    "    # encoding\n",
    "        encoder = EncoderDecoderModelEncoding()\n",
    "        s_q, hyp_batch = encoder.encoding(src_batch, parameter)\n",
    "    # decoding\n",
    "        decoder = EncoderDecoderModelDecoding()\n",
    "        if is_training:\n",
    "            return decoder.decoding(is_training, src_batch, parameter, s_q, hyp_batch, trg_batch, generation_limit)\n",
    "        else:\n",
    "            return decoder.decoding(is_training, src_batch, parameter, s_q, hyp_batch, trg_batch, generation_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=7.各値を設定 /> 7.各値を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各値を設定\n",
    "\n",
    "* 翻訳元言語の設定(学習データ)\n",
    "* 翻訳先言語の設定（学習データ）\n",
    "* 翻訳元言語の設定(テストデータ)\n",
    "* 翻訳先言語の設定（テストデータ）\n",
    "* 語彙の設定\n",
    "* 潜在空間の設定\n",
    "* 隠れ層の設定\n",
    "* 学習回数の設定\n",
    "* ミニバッチサイズの設定\n",
    "* 最大予測言語数の設定\n",
    "ベストな調整方法は経験則か力技です。グリッドサーチ、ランダムサーチ、データから推定など。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_dict = {}\n",
    "train_path = \"oda201512handson/train/\"\n",
    "test_path = \"oda201512handson/test/\"\n",
    "parameter_dict[\"source\"] = train_path + \"train1000.ja\"\n",
    "parameter_dict[\"target\"] = train_path + \"train1000.en\"\n",
    "parameter_dict[\"test_source\"] = test_path + \"test1000.ja\"\n",
    "parameter_dict[\"test_target\"] = test_path + \"test1000_hyp.en\"\n",
    "parameter_dict[\"reference_target\"] = test_path + \"test1000.en\"\n",
    "#--------Hands on  2----------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "下記の値が大きいほど扱える語彙の数が増えて表現力が上がるが計算量が爆発的に増えるので大きくしない方が良いです。\n",
    "\"\"\"\n",
    "parameter_dict[\"vocab\"] = 550\n",
    "\n",
    "\"\"\"\n",
    "この数が多くなればなるほどモデルが複雑になります。この数を多くすると必然的に学習回数を多くしないと学習は\n",
    "収束しません。\n",
    "語彙数よりユニット数の数が多いと潜在空間への写像が出来ていないことになり結果的に意味がない処理になります。\n",
    "\"\"\"\n",
    "parameter_dict[\"embed\"] = 500\n",
    "\n",
    "\"\"\"\n",
    "この数も多くなればなるほどモデルが複雑になります。この数を多くすると必然的に学習回数を多くしないと学習は\n",
    "収束しません。\n",
    "\"\"\"\n",
    "parameter_dict[\"hidden\"] = 20\n",
    "\n",
    "\"\"\"\n",
    "学習回数。基本的に大きい方が良いが大きすぎると収束しないです。\n",
    "\"\"\"\n",
    "parameter_dict[\"epoch\"] = 20\n",
    "\n",
    "\"\"\"\n",
    "ミニバッチ学習で扱うサイズです。この点は経験的に調整する場合が多いが、基本的に大きくすると学習精度が向上する\n",
    "代わりに学習スピードが落ち、小さくすると学習精度が低下する代わりに学習スピードが早くなります。\n",
    "\"\"\"\n",
    "parameter_dict[\"minibatch\"] = 64\n",
    "\n",
    "\"\"\"\n",
    "予測の際に必要な単語数の設定。長いほど多くの単語の翻訳が確認できるが、一般的にニューラル翻訳は長い翻訳には\n",
    "向いていないので小さい数値がオススメです。\n",
    "\"\"\"\n",
    "parameter_dict[\"generation_limit\"] = 256\n",
    "\n",
    "#--------Hands on  2----------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=8.実行 /> 8.実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace('initializing ...')\n",
    "wrapper.init()\n",
    "\n",
    "encoderDecoderModel = EncoderDecoderModelForward(parameter_dict)\n",
    "encoderDecoderModel.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=9.学習したモデルを使用したテスト /> 9.学習したモデルを使用したテスト "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルを使用してテスト\n",
    "\n",
    "* 学習したモデルを利用してテストデータ（日本語）を英語に翻訳しモデルに保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"ChainerMachineTranslation.021\"\n",
    "trace('initializing ...')\n",
    "wrapper.init()\n",
    "\n",
    "encoderDecoderModel = EncoderDecoderModelForward(parameter_dict)\n",
    "encoderDecoderModel.test_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=10.学習したモデルを評価 (Advanced) /> 10.学習したモデルを評価 (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルの評価するため、BLEUを算出\n",
    "\n",
    "* BlEUとは翻訳の客観的評価に使用される指標で、答えとなる文章との一致率を評価する方法を用いています。\n",
    "詳しく知りたい方は下記をご覧ください。\n",
    "http://www2.nict.go.jp/univ-com/multi_trans/member/mutiyama/corpmt/4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd_corpus = \"mteval-corpus  -e BLEU RIBES -r \" +parameter_dict[\"reference_target\"] + \" -h \" + parameter_dict[\"test_target\"]\n",
    "cmd_sentence = \"mteval-sentence  -e BLEU RIBES -r \" +parameter_dict[\"reference_target\"] + \" -h \" + parameter_dict[\"test_target\"]\n",
    "mteval_corpus = subprocess.Popen(cmd_corpus, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout_data = mteval_corpus.stdout.read()\n",
    "print(stdout_data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
